{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "ebcba6e2",
            "metadata": {},
            "source": [
                "# Improve RAG Accuracy with Finetuned Embedding Models on Amazon SageMaker\n",
                "\n",
                "The purpose of the \"Install Dependencies\" section is to guide users on setting up their environment with the necessary Python libraries to run the notebook. It should clearly explain why each library is needed in the context of fine-tuning sentence transformer models on SageMaker for improved RAG accuracy.\n",
                "\n",
                "<a id='install-dependencies'></a>\n",
                "### Install Dependencies\n",
                "\n",
                "    Before we can start fine-tuning our embedding model and deploying it on Amazon SageMaker, we need to ensure our environment has all the necessary Python libraries. These libraries provide the tools and functionalities we'll use throughout this notebook.  Let's install them step-by-step:\n",
                "\n",
                "**1. Amazon SageMaker Python SDK:**\n",
                "pip install sagemaker --upgrade\n",
                "\n",
                "\n",
                "    What it is: The Amazon SageMaker Python SDK is essential for interacting with Amazon SageMaker services from within your Python environment. It allows us to manage and deploy machine learning models, including training, deploying endpoints, and more, all within SageMaker's infrastructure.\n",
                "\n",
                "Why we need it: We'll use the SageMaker SDK to:\n",
                "\n",
                "Create a SageMaker Session to interact with AWS resources.\n",
                "\n",
                "Define and configure our training job.\n",
                "\n",
                "Deploy our fine-tuned embedding model as a SageMaker Endpoint so we can easily use it for inference (generating embeddings for our RAG system).\n",
                "\n",
                "**2. Hugging Face transformers and datasets libraries:**\n",
                "pip install transformers datasets\n",
                "\n",
                "What they are:\n",
                "\n",
                "    transformers: This library from Hugging Face is the foundation for working with pre-trained transformer models, including Sentence Transformers models. It provides classes and functions to load, use, and fine-tune these powerful models.\n",
                "\n",
                "    datasets: Also from Hugging Face, datasets makes it incredibly easy to download and manage datasets for machine learning tasks. It provides efficient data loading and processing capabilities.\n",
                "\n",
                "Why we need them:\n",
                "\n",
                "    transformers: We need it to load the pre-trained sentence-transformers/all-MiniLM-L6-v2 model, which is a transformer-based model. We'll also use it during the fine-tuning process.\n",
                "\n",
                "    datasets: While this notebook uses the Bedrock FAQ dataset, datasets is a useful general-purpose library for handling datasets. If you were using a dataset from Hugging Face Hub or needed to process data in a specific format, datasets would be very helpful. Even though we might load the Bedrock FAQ data directly, having datasets installed is good practice and might be used in more complex scenarios or if you adapt this notebook for different datasets.\n",
                "\n",
                "**3. Sentence Transformers Library:**\n",
                "pip install sentence-transformers\n",
                "\n",
                "What it is: sentence-transformers is a Python library built specifically for creating and working with sentence and text embeddings. It simplifies the process of using pre-trained models (like all-MiniLM-L6-v2) and fine-tuning them for specific tasks, especially semantic similarity and information retrieval, which are crucial for improving RAG accuracy.\n",
                "\n",
                "Why we need it: This is the core library for this notebook! We need sentence-transformers to:\n",
                "\n",
                "Load the sentence-transformers/all-MiniLM-L6-v2 model.\n",
                "\n",
                "Utilize the MultipleNegativesRankingLoss function for fine-tuning.\n",
                "\n",
                "Generate sentence embeddings for our text data.\n",
                "\n",
                "**4. (Optional but Recommended) Accelerate and Bitsandbytes (for faster training, especially on GPUs):**\n",
                "pip install accelerate bitsandbytes\n",
                "\n",
                "What they are:\n",
                "\n",
                "    accelerate: Hugging Face accelerate simplifies distributed training and mixed-precision training. It can significantly speed up the fine-tuning process, especially if you are using GPUs.\n",
                "\n",
                "bitsandbytes: This library allows for efficient memory usage during training by using techniques like 8-bit Adam optimization. It's particularly helpful when fine-tuning large models or when GPU memory is limited.\n",
                "\n",
                "Why they are recommended:\n",
                "\n",
                "    Speed up training: Fine-tuning deep learning models can be time-consuming. accelerate and bitsandbytes can drastically reduce training time, especially if you are using a SageMaker Notebook instance with a GPU or a SageMaker Training Job with GPUs.\n",
                "\n",
                "    Memory efficiency: They can help you fine-tune larger models or handle larger datasets within the available GPU memory.\n",
                "\n",
                "After running these pip install commands, ensure that all installations are successful without any errors. Once these libraries are installed, you'll be ready to move on to loading your data and starting the model fine-tuning process in the next section.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bf626f15-ce6f-4827-b6d6-781ad526774e",
            "metadata": {
                "scrolled": true,
                "tags": []
            },
            "outputs": [],
            "source": [
                "!pip install pathos==0.3.2\n",
                "!pip install datasets==2.19.2\n",
                "!pip install transformers==4.40.2\n",
                "!pip install transformers[torch]==4.40.2\n",
                "!pip install sentence_transformers==3.1.1\n",
                "!pip install accelerate==1.0.0\n",
                "!pip install sagemaker==2.224.1"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "1bc2fa78",
            "metadata": {},
            "source": [
                "<a id='load-data-and-train-the-model'></a>\n",
                "## Load Data and Train the Model\n",
                "\n",
                "The following code snippet demonstrates how to load a training dataset from a JSON file, prepare the data for training, and then fine-tune the pre-trained model. After fine-tuning, the updated model is saved.\n",
                "\n",
                "The `EPOCHS` variable determines the number of times the model will iterate over the entire training dataset during the fine-tuning process. A higher number of epochs typically leads to better convergence and potentially improved performance, but may also increase the risk of overfitting if not properly regularized.\n",
                "\n",
                "In this example, we have a small training set consisting of only 100 records. As a result, we are using a high value for the `EPOCHS` parameter. Typically, in real-world scenarios, you would have a much larger training set. In such cases, the `EPOCHS` value should be a single or two-digit number to avoid overfitting the model to the training data.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "70e1f8ae-23ad-4854-94ce-2d4d5a3e3f6d",
            "metadata": {
                "scrolled": true,
                "tags": []
            },
            "outputs": [],
            "source": [
                "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation\n",
                "from torch.utils.data import DataLoader\n",
                "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
                "import json\n",
                "\n",
                "def load_data(path):\n",
                "    \"\"\"Load the dataset from a JSON file.\"\"\"\n",
                "    with open(path, 'r', encoding='utf-8') as f:\n",
                "        data = json.load(f)\n",
                "    return data\n",
                "\n",
                "dataset = load_data(\"training.json\")\n",
                "\n",
                "\n",
                "# Load the pre-trained model\n",
                "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
                "\n",
                "# Convert the dataset to the required format\n",
                "train_examples = [InputExample(texts=[data[\"sentence1\"], data[\"sentence2\"]]) for data in dataset]\n",
                "\n",
                "# Create a DataLoader object\n",
                "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=8)\n",
                "\n",
                "# Define the loss function\n",
                "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
                "\n",
                "EPOCHS=100\n",
                "\n",
                "model.fit(\n",
                "    train_objectives=[(train_dataloader, train_loss)],\n",
                "    epochs=EPOCHS,\n",
                "    show_progress_bar=True,\n",
                ")\n",
                "\n",
                "# Save the fine-tuned model\n",
                "model.save(\"opt/ml/model/\",safe_serialization=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "03d874e8",
            "metadata": {},
            "source": [
                "<a id='create-inference-script'></a>\n",
                "## Create inference.py File\n",
                "\n",
                "To deploy and serve the fine-tuned embedding model for inference, we create an `inference.py` Python script that serves as the entry point. This script implements two essential functions: `model_fn` and `predict_fn`, as required by AWS SageMaker for deploying and using machine learning models.\n",
                "\n",
                "The `model_fn` is responsible for loading the fine-tuned embedding model and the associated tokenizer. On the other hand, the `predict_fn` takes input sentences, tokenizes them using the loaded tokenizer, and computes their sentence embeddings using the fine-tuned model. To obtain a single vector representation for each sentence, it performs mean pooling over the token embeddings, followed by normalization of the resulting embedding. Finally, the `predict_fn` returns the normalized embeddings as a list, which can be further processed or stored as required.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0251ce77-43d8-4a9c-9315-c65e3b53c1d8",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "%%writefile opt/ml/model/inference.py\n",
                "\n",
                "from transformers import AutoTokenizer, AutoModel\n",
                "import torch\n",
                "import torch.nn.functional as F\n",
                "import os\n",
                "\n",
                "def mean_pooling(model_output, attention_mask):\n",
                "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
                "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
                "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
                "\n",
                "\n",
                "def model_fn(model_dir, context=None):\n",
                "  # Load model from HuggingFace Hub\n",
                "  tokenizer = AutoTokenizer.from_pretrained(f\"{model_dir}/model\")\n",
                "  model = AutoModel.from_pretrained(f\"{model_dir}/model\")\n",
                "  return model, tokenizer\n",
                "\n",
                "def predict_fn(data, model_and_tokenizer, context=None):\n",
                "    # destruct model and tokenizer\n",
                "    model, tokenizer = model_and_tokenizer\n",
                "    \n",
                "    # Tokenize sentences\n",
                "    sentences = data.pop(\"inputs\", data)\n",
                "    encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
                "\n",
                "    # Compute token embeddings\n",
                "    with torch.no_grad():\n",
                "        model_output = model(**encoded_input)\n",
                "\n",
                "    # Perform pooling\n",
                "    sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
                "\n",
                "    # Normalize embeddings\n",
                "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
                "    \n",
                "    # return dictonary, which will be json serializable\n",
                "    return {\"vectors\": sentence_embeddings[0].tolist()}\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e875a2cc",
            "metadata": {},
            "source": [
                "<a id='upload-the-model'></a>\n",
                "## Upload the Model\n",
                "\n",
                "After creating the `inference.py` script, we package it together with the fine-tuned embedding model into a single `model.tar.gz` file. This compressed file can then be uploaded to an Amazon S3 bucket, making it accessible for deployment as a SageMaker endpoint."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "356331c8-2637-4d07-bad4-bb1e8acf88ab",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "import boto3\n",
                "import tarfile\n",
                "import os\n",
                "\n",
                "model_dir = \"opt/ml/model\"\n",
                "model_tar_path = \"model.tar.gz\"\n",
                "\n",
                "with tarfile.open(model_tar_path, \"w:gz\") as tar:\n",
                "    tar.add(model_dir, arcname=os.path.basename(model_dir))\n",
                "    \n",
                "s3 = boto3.client('s3')\n",
                "\n",
                "# Get the region name\n",
                "session = boto3.Session()\n",
                "region_name = session.region_name\n",
                "\n",
                "# Get the account ID from STS (Security Token Service)\n",
                "sts_client = session.client(\"sts\")\n",
                "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
                "\n",
                "model_path = f\"s3://sagemaker-{region_name}-{account_id}/model_trained_embedding/model.tar.gz\"\n",
                "\n",
                "bucket_name = f\"sagemaker-{region_name}-{account_id}\"\n",
                "s3_key = \"model_trained_embedding/model.tar.gz\"\n",
                "\n",
                "with open(model_tar_path, \"rb\") as f:\n",
                "    s3.upload_fileobj(f, bucket_name, s3_key)\n",
                "          "
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ecdd12ec",
            "metadata": {},
            "source": [
                "<a id='deploy-model-on-sagemaker'></a>\n",
                "## Deploy Model on SageMaker\n",
                "\n",
                "Finally, we can deploy our fine-tuned model on a SageMaker Endpoint using `SageMaker HuggingFaceModel`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "eaa9d0d1-60d8-4725-84e3-afbcfded5a88",
            "metadata": {
                "scrolled": true,
                "tags": []
            },
            "outputs": [],
            "source": [
                "from sagemaker.huggingface.model import HuggingFaceModel\n",
                "import sagemaker\n",
                "\n",
                "\n",
                "# create Hugging Face Model Class\n",
                "huggingface_model = HuggingFaceModel(\n",
                "   model_data=model_path,  # path to your trained SageMaker model\n",
                "   role=sagemaker.get_execution_role(),                                            # IAM role with permissions to create an endpoint\n",
                "   transformers_version=\"4.26\",                           # Transformers version used\n",
                "   pytorch_version=\"1.13\",                                # PyTorch version used\n",
                "   py_version='py39',                                    # Python version used\n",
                "   entry_point=\"opt/ml/model/inference.py\",\n",
                ")\n",
                "\n",
                "# deploy model to SageMaker Inference\n",
                "predictor = huggingface_model.deploy(\n",
                "   initial_instance_count=1,\n",
                "   instance_type=\"ml.m5.xlarge\"\n",
                ")\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e1fe3822",
            "metadata": {},
            "source": [
                "<a id='invoke-the-model'></a>\n",
                "## Invoke the Model\n",
                "\n",
                "You can invoke the model using the `predict` function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e1a8c26f-beae-416d-8233-0bda0ebca4ac",
            "metadata": {
                "scrolled": true,
                "tags": []
            },
            "outputs": [],
            "source": [
                "# example request: you always need to define \"inputs\"\n",
                "data = {\n",
                "   \"inputs\": \"Are Agents fully managed?.\"\n",
                "}\n",
                "\n",
                "# request\n",
                "predictor.predict(data)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "b27e4d85",
            "metadata": {},
            "source": [
                "<a id='compare-predictions'></a>\n",
                "## Compare Predictions\n",
                "\n",
                "To illustrate the impact of fine-tuning, we can compare the cosine similarity scores between two semantically related sentences using both the original pre-trained model and the fine-tuned model. A higher cosine similarity score indicates that the two sentences are more semantically similar, as their embeddings are closer in the vector space."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "221624b7-acdd-4c31-b7bb-69d41e51c35b",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "from sentence_transformers import SentenceTransformer, util\n",
                "\n",
                "pretrained_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
                "\n",
                "\n",
                "sentences = [\n",
                "    \"What are Agents, and how can they be used?\"\n",
                "    , \n",
                "    \"Agents for Amazon Bedrock are fully managed capabilities that automatically break down tasks, create an orchestration plan, securely connect to company data through APIs, and generate accurate responses for complex tasks like automating inventory management or processing insurance claims.\"\n",
                "]\n",
                "\n",
                "#Compute embedding for both lists\n",
                "embedding_x= pretrained_model.encode(sentences[0], convert_to_tensor=True)\n",
                "embedding_y = pretrained_model.encode(sentences[1], convert_to_tensor=True)\n",
                "\n",
                "util.pytorch_cos_sim(embedding_x, embedding_y)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "55be0ba2-7647-4e94-b323-1a059f1b1280",
            "metadata": {
                "tags": []
            },
            "outputs": [],
            "source": [
                "from sentence_transformers import SentenceTransformer, util\n",
                "\n",
                "data1 = {\n",
                "   \"inputs\": \n",
                "    \"What are Agents, and how can they be used?\"\n",
                "}\n",
                "\n",
                "data2 = {\n",
                "   \"inputs\": \n",
                "    \"Agents for Amazon Bedrock are fully managed capabilities that automatically break down tasks, create an orchestration plan, securely connect to company data through APIs, and generate accurate responses for complex tasks like automating inventory management or processing insurance claims.\"\n",
                "}\n",
                "\n",
                "\n",
                "\n",
                "el1 = predictor.predict(data1)\n",
                "el2 = predictor.predict(data2)\n",
                "\n",
                "util.pytorch_cos_sim(el1[\"vectors\"], el2[\"vectors\"])\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9e7b4e61-13a0-46e6-beec-5677ba48158a",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "conda_python3",
            "language": "python",
            "name": "conda_python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
